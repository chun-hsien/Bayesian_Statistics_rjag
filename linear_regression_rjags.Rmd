---
title: "bayesian linear regression"
author: "Chun Hsien Wu"
date: "2021年6月3日"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### Bayesian Linear Regression Using R and JAGS 

使用 `car` 套件中的 `Leinhardt` 資料庫 

<http://rmarkdown.rstudio.com>.

 **Knit** 

```{r cars}
library("car")
data("Leinhardt")
?Leinhardt
head(Leinhardt)
```
Using  `str`function to understand `Leinhardt` dataset contained variables.

```{r}
str(Leinhardt)
```

```{r}
pairs(Leinhardt)
```

infant mortality and income doesn't like a linear relationshiop

```{r}
plot(infant ~ income, data=Leinhardt)
```
In fact, both of these variables are extremely right skewed(Matthew Heiner). Let's look at a histogram of each one individually.

```{r}
hist(Leinhardt$infant)
```

```{r}
hist(Leinhardt$income)
```

A linear regression is not appropriate for these variables.
We could try looking at this on the log scale. Let's recreate these plots now with variables that are transformed.

```{r}
Leinhardt$loginfant = log(Leinhardt$infant)
Leinhardt$logincome = log(Leinhardt$income)
plot(loginfant ~ logincome, data=Leinhardt)
```

### Modeling
Now using logcincome as the independent variable and loginfant as the dependent variable to fit linear regression.

```{r}
lmod = lm(loginfant ~ logincome, data=Leinhardt)
summary(lmod)
```
Under non-informative flat prior, here are the estimates, the posterior mean estimates for the coefficients. 

These estimates are relative to theri standard error, or in these case, standard deviation of the posterior distribution. And they appear to be very statisstatistically significant.

Residual standard error gives us an estimate of the left over variance after fitting the model. The R squared statistics tell us how much of the variability
is explained by the linear model, in this case about half.

From the results above, we can see that 4 observations were deleted. They were not used in the model because they contained missing values.

Now delete the observations that have missing values before we perform analysis. we can save a new dataset  called it dat. To do this in R, we use na.omit and we give it the data set.

```{r}
dat = na.omit(Leinhardt)
```


## Bayesian Linear Regression using JAGS
Now, load rjags in R
```{r}
library("rjags")
```
Below are jags codes for the infant motality data.
```{r}
mod1_string = " model {
    for (i in 1:n) {
        y[i] ~ dnorm(mu[i], prec)
        mu[i] = b[1] + b[2]*log_income[i] 
    }
    
    for (i in 1:2) {
        b[i] ~ dnorm(0.0, 1.0/1.0e6)
    }
    
    prec ~ dgamma(5/2.0, 5*10.0/2.0)
    sig2 = 1.0 / prec
    sig = sqrt(sig2)
} "

set.seed(72)
data1_jags = list(y=dat$loginfant, n=nrow(dat), 
              log_income=dat$logincome)

params1 = c("b", "sig")

inits1 = function() {
    inits = list("b"=rnorm(2,0.0,100.0), "prec"=rgamma(1,1.0,1.0))
}

mod1 = jags.model(textConnection(mod1_string), data=data1_jags, inits=inits1, n.chains=3)
update(mod1, 1000) # burn-in

mod1_sim = coda.samples(model=mod1,
                        variable.names=params1,
                        n.iter=5000)

mod1_csim = do.call(rbind, mod1_sim) # combine multiple chains
```


### Checking MCMC convergence
We have to judge whether the chains have reached convergence or not by three methods: 
We start with trace plot

```{r}
plot(mod1_sim)
```

The first one is Gelman & Rubin diagostics
```{r}
gelman.diag(mod1_sim)
```

The protential scale reduction factors for the three different parameters is very close to 1, indicating that the chains have converged.

Now, let us look at autocorrelation

```{r}
autocorr.diag(mod1_sim)
```

We can see very high auto correlation with the initial lags in the intercept term, as well as our second data term for the slope.
Now let us check the effective sample size:

```{r}
effectiveSize(mod1_sim)
```

Recall, we ran the chain for 5000 iterations for 3 different chains, so we should have about 15000 samples. Out of those 15000 samples, 
our effective sample size for beta_1, or the intercept, is only 350 and so on. The sigma parameter mixed very well, our coefficents no so much.

Look at a posterior summary from this model:

```{r}
summary(mod1_sim)
```

We use the first 1000 samples as a burnin, then we saved the next 5000 sampls.
Look at a posterior summary from this model.

### Residuals
The last thing we'll do in this segment is to check the residuals. Residuals are defined as the difference between the response, 
the actual observation, and the model's prediction for each value.
Residuals are extremely important in linear modeling, since residuals can reveal violations in the assumptions we made to specify the model.
In particular, we are looking for any sign that the model is not linear, that is not normally distributed, or that the observarions are not independent from each other, 
at least conditionally on the explanatory variables.

Let's look at what would have happen if we fit the reference linear model to the un-transformed variables.
It didn't look like linear model was a good idea here.

```{r}
lmod0 = lm(infant ~ income, data = Leinhardt)
plot(resid(lmod))
```
It gives us residuals on the y-axis and index on the x-axis. The index refers to which row of the dataset it came from. 
If the data points were not independent from each other, we might see a pattern in this plot.
Another very important look at the residuals is if we plot the prediction of the model:

```{r}
plot(predict(lmod0), resid(lmod0))
```

In this plot, we also don't want to see any patterns, we want to see essentially randomness. Which is **not** the case in this plot.
In this plot, we also don't want to see any patterns, we want to see essentially randomness. Which is not the case in this plot.
First of all, we can see a downward trend, as the value of the prediciton from the model get higher, the residuals get smaller.

And the assumption that the residual variance is the same for all observartions appear to be wildly incorrect in this case.For all of these predicted values down here the variance in the residuals is quite small, but for large predicted values, we have large variance.

#### normality
We can check the assumption of normality using the qqnorm plot on the residuals

```{r}
qqnorm(resid(lmod0))
```
These plot shows the theoretical quantiles or percetiles of an actual normal distribution on the x-axis with the sample quantiles of the residuals on the y-axis.
If the residuals actually came from a norml disbribution, the points on this plot would essentially follow a straight line.
In this case, we have a curvature going up that increases and get more extreme at the high values.

This indicates that the residuals have a distribution that is right skewed and not normal.

Return to our model fit to the log transformed variables. we'll use our residual from our Bayesian model that we fit in JAGS.
Predictions from Bayesian  models come as posterior predictive distributions. So in reality, with the Bayesian model all residuals, or
all residuals or each residual, would have its own distribution. Now, let's look at the residuals that come only from predictions based on the posterior means.
First, create the design matrix, the matrix that contains the explanatory variables, do that by cbinding, that is combining  columns, first, of the term that goes with the intercept.

Return to our model fit to the log transformed variables. we'll use our residual from our Bayesian model that we fit in JAGS.
Predictions from Bayesian  models come as posterior predictive distributions. So in reality, with the Bayesian model all residuals, or
all residuals or each residual, would have its own distribution. Now, let's look at the residuals that come only from predictions based on the posterior means.
First, create the design matrix, the matrix that contains the explanatory variables, do that by cbinding, that is combining  columns, first, of the term that goes with the intercept.

We will repeat the number 1 n times, where n comes from data1_jags, we have saved the variables earlier.

```{r}
data1_jags$n
```

You can also embed plots, for example:
```{r pressure, echo=FALSE}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
